{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import csv \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r\"C:\\Users\\Farz Jamal\\Downloads\\AI-Sentiment-Analysis-on-IMDB-Dataset-master\\aclImdb\\train\\\\\" # source data\n",
    "test_path = r\"C:\\Users\\Farz Jamal\\Downloads\\AI-Sentiment-Analysis-on-IMDB-Dataset-master\\test\\imdb_te.csv\" # test data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "IMDB_DATA_PREPROCESS explores the neg and pos folders from aclImdb/train and creates a output_file in the required format\n",
    "Inpath - Path of the training samples \n",
    "Outpath - Path were the file has to be saved \n",
    "Name  - Name with which the file has to be saved \n",
    "Mix - Used for shuffling the data \n",
    "'''\n",
    "def imdb_data_preprocess(inpath, outpath=\"./\", name=\"imdb_training.csv\", mix=False):\n",
    "\n",
    "\n",
    "    stopwords = open(r\"C:\\Users\\Farz Jamal\\Downloads\\AI-Sentiment-Analysis-on-IMDB-Dataset-master\\stopwords.en.txt\", 'r' , encoding=\"ISO-8859-1\").read()\n",
    "    stopwords = stopwords.split(\"\\n\")   #stopwords is a list of Stop Words\n",
    "\n",
    "    indices,text,rating = [],[],[]\n",
    "\n",
    "    i =  0 \n",
    "\n",
    "    for filename in os.listdir(inpath+\"pos\"):\n",
    "        data = open(inpath+\"pos/\"+filename, 'r' , encoding=\"ISO-8859-1\").read()\n",
    "        data = remove_stopwords(data, stopwords)\n",
    "        indices.append(i)\n",
    "        text.append(data)\n",
    "        rating.append(\"1\")\n",
    "        i+=1\n",
    "\n",
    "    for filename in os.listdir(inpath+\"neg\"):\n",
    "        data = open(inpath+\"neg/\"+filename, 'r' , encoding=\"ISO-8859-1\").read()\n",
    "        data = remove_stopwords(data, stopwords)\n",
    "        indices.append(i)\n",
    "        text.append(data)\n",
    "        rating.append(\"0\")\n",
    "        i+=1\n",
    "\n",
    "    Dataset = list(zip(indices,text,rating))   #Each element of this list will be a tuple of the form (index,text,rating)\n",
    "\n",
    "    if mix:\n",
    "        np.random.shuffle(Dataset)    #shuffles the dataset\n",
    "\n",
    "    df = pd.DataFrame(data = Dataset, columns=['row_Number', 'text', 'polarity'])\n",
    "    df.to_csv(outpath+name, index=False, header=True)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "REMOVE_STOPWORDS takes a sentence and a list of stopwords as inputs.Then returns the sentence without any stopwords. \n",
    "Sentence - The input from which the stopwords have to be removed\n",
    "Stopwords - A list of stopwords  \n",
    "'''\n",
    "def remove_stopwords(sentence, stopwords):\n",
    "    sentencewords = sentence.split()     #splits words of sentence into a list\n",
    "    resultwords  = [word for word in sentencewords if word.lower() not in stopwords]\n",
    "    result = ' '.join(resultwords)\n",
    "    return result\n",
    "\n",
    "\n",
    "'''\n",
    "UNIGRAM_PROCESS takes the data to be fit as the input and returns a vectorizer of the unigram as output \n",
    "Data - The data for which the unigram model has to be fit \n",
    "'''\n",
    "def unigram_process(data):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer = vectorizer.fit(data)\n",
    "    return vectorizer\n",
    "\n",
    "\n",
    "'''\n",
    "BIGRAM_PROCESS takes the data to be fit as the input and returns a vectorizer of the bigram as output \n",
    "Data - The data for which the bigram model has to be fit \n",
    "'''\n",
    "def bigram_process(data):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,2))    #preserves one word as well as two word sequence thanks to the ngram_range parameter\n",
    "    vectorizer = vectorizer.fit(data)\n",
    "    return vectorizer\n",
    "\n",
    "\n",
    "'''\n",
    "TFIDF_PROCESS takes the data to be fit as the input and returns a vectorizer of the tfidf as output \n",
    "Data - The data for which the bigram model has to be fit \n",
    "'''\n",
    "def tfidf_process(data):\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer \n",
    "    transformer = TfidfTransformer()\n",
    "    transformer = transformer.fit(data)\n",
    "    return transformer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "RETRIEVE_DATA takes a CSV file as the input and returns the corresponding arrays of labels and data as output. \n",
    "Name - Name of the csv file \n",
    "Train - If train is True, both the data and labels are returned. Else only the data is returned \n",
    "'''\n",
    "def retrieve_data(name=\"imdb_training.csv\", train=True):\n",
    "    import pandas as pd \n",
    "    data = pd.read_csv(name,header=0, encoding = 'ISO-8859-1')\n",
    "    X = data['text']\n",
    "\n",
    "    if train:\n",
    "        Y = data['polarity']\n",
    "        return X, Y\n",
    "\n",
    "    return X\n",
    "\n",
    "'''\n",
    "STOCHASTIC_DESCENT applies Stochastic on the training data and returns the predicted labels \n",
    "Xtrain - Training Data\n",
    "Ytrain - Training Labels\n",
    "Xtest - Test Data \n",
    "'''\n",
    "def stochastic_descent(Xtrain, Ytrain, Xtest):\n",
    "    from sklearn.linear_model import SGDClassifier \n",
    "    clf = SGDClassifier(loss=\"hinge\", penalty=\"l1\", n_iter=25)        #hinge loss , therefore using a SVM\n",
    "    print (\"Fitting SGD\")\n",
    "    clf.fit(Xtrain, Ytrain)\n",
    "    print (\"Predicting using SGD\")\n",
    "    Ytest = clf.predict(Xtest)\n",
    "    return Ytest\n",
    "\n",
    "\n",
    "'''\n",
    "ACCURACY finds the accuracy in percentage given the training and test labels \n",
    "Ytrain - One set of labels \n",
    "Ytest - Other set of labels \n",
    "'''\n",
    "def accuracy(Ytrain, Ytest):\n",
    "    assert (len(Ytrain)==len(Ytest))\n",
    "    num =  sum([1 for i, word in enumerate(Ytrain) if Ytest[i]==word])\n",
    "    n = len(Ytrain)  \n",
    "    return (num*100)/n\n",
    "\n",
    "\n",
    "'''\n",
    "WRITE_TXT writes the given data to a text file \n",
    "Data - Data to be written to the text file \n",
    "Name - Name of the file \n",
    "'''\n",
    "def write_txt(data, name):\n",
    "    data = ''.join(str(word) for word in data)\n",
    "    file = open(name, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing the training_data--\n",
      "Done with preprocessing. Now, will retreieve the training data in the required format\n",
      "Retrieved the training data. Now will retrieve the test data in the required format\n",
      "Retrieved the test data. Now will initialize the model \n",
      "\n",
      "\n",
      "-----------------------ANALYSIS ON THE TRAINING DATA---------------------------\n",
      "Fitting the unigram model\n",
      "After fitting \n",
      "\n",
      "\n",
      "Fitting the bigram model\n",
      "After fitting \n",
      "Applying the stochastic descent\n",
      "Fitting SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using SGD\n",
      "Done with  stochastic descent\n",
      "Accuracy for the Bigram Model on the training data  94.052\n",
      "\n",
      "\n",
      "Fitting the tfidf for unigram model\n",
      "After fitting TFIDF\n",
      "Applying the stochastic descent\n",
      "Fitting SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using SGD\n",
      "Done with  stochastic descent\n",
      "Accuracy for the Unigram TFIDF Model is  88.792\n",
      "\n",
      "\n",
      "Fitting the tfidf for bigram model\n",
      "After fitting TFIDF\n",
      "Applying the stochastic descent\n",
      "Fitting SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using SGD\n",
      "Done with  stochastic descent\n",
      "Accuracy for the Unigram TFIDF Model is  86.704\n",
      "\n",
      "\n",
      "-----------------------ANALYSIS ON THE TEST DATA ---------------------------\n",
      "Unigram Model on the Test Data--\n",
      "Applying the stochastic descent\n",
      "Fitting SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using SGD\n",
      "Done with  stochastic descent\n",
      "\n",
      "\n",
      "Bigram Model on the Test Data--\n",
      "Applying the stochastic descent\n",
      "Fitting SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using SGD\n",
      "Done with  stochastic descent\n",
      "\n",
      "\n",
      "Unigram TF Model on the Test Data--\n",
      "Applying the stochastic descent\n",
      "Fitting SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using SGD\n",
      "Done with  stochastic descent\n",
      "\n",
      "\n",
      "Bigram TF Model on the Test Data--\n",
      "Applying the stochastic descent\n",
      "Fitting SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using SGD\n",
      "Done with  stochastic descent\n",
      "\n",
      "\n",
      "Total time taken =  312.6678087711334  seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    start = time.time()\n",
    "    print (\"Preprocessing the training_data--\")\n",
    "    imdb_data_preprocess(inpath=train_path, mix=True)\n",
    "    print (\"Done with preprocessing. Now, will retreieve the training data in the required format\")\n",
    "    [Xtrain_text, Ytrain] = retrieve_data()\n",
    "    print (\"Retrieved the training data. Now will retrieve the test data in the required format\")\n",
    "    Xtest_text = retrieve_data(name=test_path, train=False)\n",
    "    print (\"Retrieved the test data. Now will initialize the model \\n\\n\")\n",
    "\n",
    "\n",
    "    print (\"-----------------------ANALYSIS ON THE TRAINING DATA---------------------------\")\n",
    "    uni_vectorizer = unigram_process(Xtrain_text)\n",
    "    print (\"Fitting the unigram model\")\n",
    "    Xtrain_uni = uni_vectorizer.transform(Xtrain_text)\n",
    "    print (\"After fitting \")\n",
    "    print (\"\\n\")\n",
    "\n",
    "    bi_vectorizer = bigram_process(Xtrain_text)\n",
    "    print (\"Fitting the bigram model\")\n",
    "    Xtrain_bi = bi_vectorizer.transform(Xtrain_text)\n",
    "    print (\"After fitting \")\n",
    "    print (\"Applying the stochastic descent\")\n",
    "    Y_bi = stochastic_descent(Xtrain_bi, Ytrain, Xtrain_bi)\n",
    "    print (\"Done with  stochastic descent\")\n",
    "    print (\"Accuracy for the Bigram Model on the training data \", accuracy(Ytrain, Y_bi))\n",
    "    print (\"\\n\")\n",
    "\n",
    "    uni_tfidf_transformer = tfidf_process(Xtrain_uni)\n",
    "    print (\"Fitting the tfidf for unigram model\")\n",
    "    Xtrain_tf_uni = uni_tfidf_transformer.transform(Xtrain_uni)\n",
    "    print (\"After fitting TFIDF\")\n",
    "    print (\"Applying the stochastic descent\")\n",
    "    Y_tf_uni = stochastic_descent(Xtrain_tf_uni, Ytrain, Xtrain_tf_uni)\n",
    "    print (\"Done with  stochastic descent\")\n",
    "    print (\"Accuracy for the Unigram TFIDF Model is \", accuracy(Ytrain, Y_tf_uni))\n",
    "    print (\"\\n\")\n",
    "\n",
    "\n",
    "    bi_tfidf_transformer = tfidf_process(Xtrain_bi)\n",
    "    print (\"Fitting the tfidf for bigram model\")\n",
    "    Xtrain_tf_bi = bi_tfidf_transformer.transform(Xtrain_bi)\n",
    "    print (\"After fitting TFIDF\")\n",
    "    print (\"Applying the stochastic descent\")\n",
    "    Y_tf_bi = stochastic_descent(Xtrain_tf_bi, Ytrain, Xtrain_tf_bi)\n",
    "    print (\"Done with  stochastic descent\")\n",
    "    print (\"Accuracy for the Unigram TFIDF Model is \", accuracy(Ytrain, Y_tf_bi))\n",
    "    print (\"\\n\")\n",
    "\n",
    "\n",
    "    print (\"-----------------------ANALYSIS ON THE TEST DATA ---------------------------\")\n",
    "    print (\"Unigram Model on the Test Data--\")\n",
    "    Xtest_uni = uni_vectorizer.transform(Xtest_text)\n",
    "    print (\"Applying the stochastic descent\")\n",
    "    Ytest_uni = stochastic_descent(Xtrain_uni, Ytrain, Xtest_uni)\n",
    "    write_txt(Ytest_uni, name=\"unigram.output.txt\")\n",
    "    print (\"Done with  stochastic descent\")\n",
    "    print (\"\\n\")\n",
    "\n",
    "\n",
    "    print (\"Bigram Model on the Test Data--\")\n",
    "    Xtest_bi = bi_vectorizer.transform(Xtest_text)\n",
    "    print (\"Applying the stochastic descent\")\n",
    "    Ytest_bi = stochastic_descent(Xtrain_bi, Ytrain, Xtest_bi)\n",
    "    write_txt(Ytest_bi, name=\"bigram.output.txt\")\n",
    "    print (\"Done with  stochastic descent\")\n",
    "    print (\"\\n\")\n",
    "\n",
    "    print (\"Unigram TF Model on the Test Data--\")\n",
    "    Xtest_tf_uni = uni_tfidf_transformer.transform(Xtest_uni)\n",
    "    print (\"Applying the stochastic descent\")\n",
    "    Ytest_tf_uni = stochastic_descent(Xtrain_tf_uni, Ytrain, Xtest_tf_uni)\n",
    "    write_txt(Ytest_tf_uni, name=\"unigramtfidf.output.txt\")\n",
    "    print (\"Done with  stochastic descent\")\n",
    "    print (\"\\n\")\n",
    "\n",
    "    print (\"Bigram TF Model on the Test Data--\")\n",
    "    Xtest_tf_bi = bi_tfidf_transformer.transform(Xtest_bi)\n",
    "    print (\"Applying the stochastic descent\")\n",
    "    Ytest_tf_bi = stochastic_descent(Xtrain_tf_bi, Ytrain, Xtest_tf_bi)\n",
    "    write_txt(Ytest_tf_bi, name=\"bigramtfidf.output.txt\")\n",
    "    print (\"Done with  stochastic descent\")\n",
    "    print (\"\\n\")\n",
    "    \n",
    "    print (\"Total time taken = \", time.time()-start, \" seconds\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
